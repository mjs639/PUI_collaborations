{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pylab as pl\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import json # used for reading json files\n",
    "import urllib2 #used for reading URLs\n",
    "import seaborn #used to beautify the plots\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "%pylab inline\n",
    "plt.style.use('seaborn-darkgrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!curl -o http://www……….ecu will download the data from the link\n",
    "!mv NAMEOFDATA.csv $PUIDATA -- will move the data to the correct directory\n",
    "!unzip ZIPFILE.zip -d $PUIDATA\n",
    "#This unzips the data, and directs it (-d) to PUIdata via the variable $PUIDATA\n",
    "\n",
    "#If data is already housed in CUSP Data Facility, you do not need to download it, you can simply read it in your \n",
    "#terminal, and then can make changes to it and then push it up\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/fedhere/PUI2017_fb55/blob/master/BashCommands.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types\n",
    "\n",
    "Dependent vs Independent variable -- independent variable ALWAYS goes on the X axis\n",
    "Plot “the dependent variable” against “the independent variable”\n",
    "\n",
    "### Levels of variable\n",
    "##### Qualitative variable\n",
    "* No ordering\n",
    "* Precinct, state, gender,\n",
    "* Also called ‘nominal’, ‘categorical’\n",
    "\n",
    "##### Quantitative variables\n",
    "* Ordering is meaningful\n",
    "* Time, distance, age, length, intensity, satisfaction\n",
    "* Can be continuous (time) or discrete (number of crimes)\n",
    "* Age is NOT discrete but often coded as such\n",
    "* Most data are represented as discrete because of computing limitations  \n",
    "\n",
    "##### Discrete data may be\n",
    "* Counts - number of bacteria at time t in section A\n",
    "* Ordinals - survey responses\n",
    "\n",
    "##### Continuous data may be\n",
    "* Continuous ordinal - earthquakes\n",
    "* Interval - temperature, interval size is preserved \n",
    "* Ratio - car speed, 0 is naturally defined\n",
    "\n",
    "##### Data may also be\n",
    "* Censored (age > 90)\n",
    "* Missing (NaN, prefer not to answer)\n",
    "* In many cases you will have to decide what to do with missing data. It may be an arbitrary choice, should be an informed decision. Maybe replace it with the mean, or the median, though doing the mean will change the standard deviation of the data\n",
    "\n",
    "##### Data definitions\n",
    "* Data = observations that have been collected\n",
    "* Population = complete body of subjects we want to infer about\n",
    "* Sample = subset of population we actually studied\n",
    "* Census = collection of data from entire population\n",
    "* Parameter = numerical value describing an attribute of the population\n",
    "* Statistics = numerical value describing an attribute of the sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first and second lines should always be:\n",
    "fig = plt.figure(figsize = (X,Y))\n",
    "fig.suptitle('Title', fontsize=Z)\n",
    "\n",
    "#Subplots\n",
    "Subplot\n",
    "ax2 = fig.add_subplot(XYZ) \n",
    "X = row\n",
    "Y = column\n",
    "Z = space\n",
    "\n",
    "\n",
    "#Plotting The Distribution\n",
    "fig = plt.figure(figsize = (10,8))\n",
    "fig.suptitle('Title for Plot', fontsize=16)\n",
    "plt.xlabel('X Axis Label')\n",
    "plt.ylabel('Y Axis Label')\n",
    "plt.plot(X, Y, linestyle=\"\",marker=\"o\")\n",
    "plt.show() \n",
    "\n",
    "#Plotting with axes/Pandas\n",
    "ax = dfRed.plot.scatter(x='df Column', y='df Column', figsize=(12,8),\n",
    "                title='TITLE')\n",
    "ax.set_xlabel(\"X AXIS\")\n",
    "ax.set_ylabel(\"Y AXIS\")\n",
    "ax.set_xlim(xmin=1000, xmax=1e10)\n",
    "ax.set_ylim(ymin=1, ymax=1000)\n",
    "\n",
    "\n",
    "#Plotting with both pandas and pyplot\n",
    "fig = figsize(15,5)\n",
    "femaleAgeCount.plot(kind=\"bar\", color = 'r', alpha=.6)\n",
    "maleAgeCount.plot(kind=\"bar\", color = 'c', alpha=.5)\n",
    "pl.xlabel('Age')\n",
    "pl.ylabel('Fraction of Riders')\n",
    "pl.title('Citibike');\n",
    "\n",
    "#Histogram\n",
    "plt.hist(zscores, alpha = .5)\n",
    "plt.hist(N_0, alpha = .5);\n",
    "\n",
    "#Scatter with Error Bar\n",
    "fig = pl.figure(figsize=(10,8)) \n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter((df['GDP'] / 1e9), df['Number of mass shootings'])\n",
    "ax.errorbar((df['GDP'] / 1e9), df['Number of mass shootings'], \n",
    "            yerr = np.sqrt(df['Number of mass shootings'] * 1.0), \n",
    "            fmt = '.')\n",
    "\n",
    "#Scatter Matrix\n",
    "scatter_matrix (df, s=300, figsize=(13, 13));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Showing the head\n",
    "df.head()\n",
    "\n",
    "#DROPPING COLUMNS\n",
    "df_new = df.drop(['Per Capita(Gallons per person per day)'], axis=1, level=None, inplace=False, errors='raise')\n",
    "#or\n",
    "df_new = df[df.columns[0:2]]\n",
    "#or\n",
    "df.drop(['tripduration', 'starttime', 'stoptime', 'start station id',\n",
    "      'start station name', 'start station latitude',\n",
    "      'start station longitude', 'end station id', 'end station name',\n",
    "      'end station latitude', 'end station longitude', 'bikeid', 'usertype'\n",
    "       ], axis=1, inplace=True)\n",
    "#By setting inplace=True, we do not have to create a new dataframe. This tells the machine to drop them from the \n",
    "#already existing df. If we want to retain the columns for later, we can use the method above where we give the\n",
    "#dropped dataframe a new name and set inplace=False\n",
    "#or\n",
    "df = df.drop(df.index[df.gender == 0])\n",
    "\n",
    "\n",
    "#CHANGING COLUMN NAMES\n",
    "df = df.rename(columns={'OLD NAME': 'NEW NAME', 'OLD NAME 2': 'NEW NAME 2'})\n",
    "\n",
    "\n",
    "#ADDING A COLUMN, IN THIS CASE A DATETIME ONE\n",
    "df['NEW COLUMN'] = pd.to_datetime(dF['NS Date'])\n",
    "\n",
    "#USING GROUPBY\n",
    "gencount = df['gender'].value_counts() \n",
    "femaleAgeCount = (df['age'][df['gender'] == 2].groupby([df['age']]).count())/gencount[2]\n",
    "maleAgeCount = (df['age'][df['gender'] == 1].groupby([df['age']]).count())/gencount[1]\n",
    "#or\n",
    "dfAvg = df.groupby('gender', as_index=False).age.mean()\n",
    "\n",
    "#EXTRACTING A COLUMN\n",
    "df.ColumnName\n",
    "#or\n",
    "df['ColumnName']\n",
    "#or\n",
    "df.ColumnName.values() #this extracts the column values in array form\n",
    "\n",
    "#CLEANING YOUR DATA, from Lab from Lecture 2\n",
    "\n",
    "data['Gender'].replace('Female',0)\n",
    "#The above line will replace all instances of Female with 0. This will NOT do it to the dataset... \n",
    "#it only does it in place. If you want to replace it in the dataset, you must do the following\n",
    "data['Gender'] = data['Gender'].replace('Female',0)\n",
    "data['Gender'] = data['Gender'].replace('Male',1)\n",
    "\n",
    "#USING THE MAP FUNCTION\n",
    "dict1 = {\"Lots\": 3, \"Moderate\": 2, \"Little\":1, \"Very Little\": 0}\n",
    "#we can replace the data:\n",
    "data['Computer Experience'].replace(dict1)\n",
    "#or make a new column:\n",
    "data['Computer Exp'] = data['Computer Experience'].replace(dict1)\n",
    "\n",
    "#We can also use map to iterate over a series\n",
    "def string_to_height(X):\n",
    "    \"\"\"This function will do something\"\"\"\n",
    "    \n",
    "    height = X.split(\"'\")\n",
    "    feet = int(height[0])\n",
    "    inches = int(height[1].strip('\"'))\n",
    "    return feet + (inches / 12.)\n",
    "\n",
    "data['Height'] = data.Height.map(string_to_height)\n",
    "\n",
    "#CONCATENATE THE DATA (add one df to the bottom of another)\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "#MERGE THE DATA\n",
    "df = pd.merge(merge2, df3, on='Country') \n",
    "\n",
    "#DROP NON VALUES\n",
    "df.dropna()\n",
    "\n",
    "#CONVERT TO NUMBERS\n",
    "df['COLUMN'] = pd.to_numeric(df['COLUMN'], errors='coerce')\n",
    "\n",
    "#CUTTING DATA\n",
    "dfCut = df[(df.nrgTotal > 1000) * (df.UnitsTotal>=10) * (df.UnitsTotal<1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the Data using urllib2 and json files  \n",
    "url = \"http://bustime.mta.info/api/siri/vehicle-monitoring.json?key=\" + mta_key + \n",
    "        \"&VehicleMonitoringDetailLevel=calls&LineRef=\" + bus_line\n",
    "response = urllib2.urlopen(url)\n",
    "data = response.read()\n",
    "data = json.loads(data)\n",
    "\n",
    "#Creating a dataframe from data\n",
    "df = pd.DataFrame(Data)\n",
    "\n",
    "#Import using environmental variable\n",
    "df = pd.read_csv(os.getenv('PUIDATA') + \"FILE NAME\")\n",
    "\n",
    "#Reading a CSV from a zipped file\n",
    "datestring = \"201707\"\n",
    "df = pd.read_csv('/gws/open/Student/citibike' + \"/\" + datestring + '-citibike-tripdata.csv.zip', compression='zip')\n",
    "\n",
    "#USING VALUES SQUEEZE TO SHAPE THE DF\n",
    "df = pd.read_csv(\"SITE LINK\", header=None).values.squeeze()\n",
    "\n",
    "#FINDING DATA IN THE GWS\n",
    "!ls /gws/open/Student/citibike\n",
    "\n",
    "#SKIP ROWS\n",
    "df = pd.read_csv(\"URL\", skiprows=[0,1,2,3], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quick tricks\n",
    "dictionary.keys() -- shows you the keys\n",
    "dictionary.values() -- shows you the values\n",
    "dictionary.items() -- shows the key/value pairing\n",
    "\n",
    "\n",
    "\n",
    "#Using Dictionaries to create multiple random distributions, from HW3_assign1\n",
    "\n",
    "#Setting the seed, the dictionary key, and key variables\n",
    "np.random.seed(50)\n",
    "mydict['chisq'] = {} \n",
    "df = mymean\n",
    "\n",
    "#Filling the dictionary\n",
    "for n in mysize:\n",
    "    mydict['chisq'][n] = np.random.chisquare(df, size = n)\n",
    "\n",
    "#Saving the means in my dictionary\n",
    "mydict['chisq']['means'] = {}\n",
    "\n",
    "#Plotting the Means\n",
    "axChisq_means = pl.figure(figsize=(15,8)).add_subplot(111)\n",
    "for nn in mydict['chisq']:\n",
    "    if not type(nn) == str:\n",
    "        mydict['chisq']['means'][nn] = mydict['chisq'][nn].mean()\n",
    "        axChisq_means.plot(nn, mydict['chisq']['means'][nn], 'o')\n",
    "        axChisq_means.set_xlabel('sample size', fontsize=18)\n",
    "        axChisq_means.set_ylabel('sample mean', fontsize=18)\n",
    "        axChisq_means.set_title('Chi squared', fontsize=18)\n",
    "        axChisq_means.plot([min(mysize), max(mysize)], [mymean, mymean], 'k')\n",
    "        \n",
    "        \n",
    "        \n",
    "#PLOTTING ALL THE MEANS\n",
    "allmeans = list(mydict['chisq']['means'].values() + \n",
    "               mydict['norm']['means'].values() +\n",
    "               mydict['pois']['means'].values() + \n",
    "               mydict['binom']['means'].values() +\n",
    "               mydict['gumb']['means'].values()\n",
    "               )\n",
    "\n",
    "pl.figure(figsize=(15, 15))\n",
    "pl.hist(allmeans,bins=50)\n",
    "pl.xlabel('sample mean', fontsize = 15)\n",
    "pl.ylabel('Number of samples', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for loop with counter at the top, from HW2 testing environment\n",
    "\n",
    "busno = 0\n",
    "for i in indbus:\n",
    "    longitude = str(i['MonitoredVehicleJourney']['VehicleLocation']['Longitude'])\n",
    "    latitude = str(i['MonitoredVehicleJourney']['VehicleLocation']['Latitude'])\n",
    "    print(\"Bus \" + str(busno) + \" is at latitude \" + latitude + \" and longitude \" + longitude)\n",
    "    busno += 1\n",
    "    \n",
    "#for loop with dictionary embedded and empty list outside of the loop, from HW2 testing environment\n",
    "\n",
    "datalist = []\n",
    "for i in indbus:\n",
    "    dict = {}\n",
    "    #dict['busno'] = busno\n",
    "    dict['Longitude'] = str(i['MonitoredVehicleJourney']['VehicleLocation']['Longitude'])\n",
    "    dict['Latitude'] = str(i['MonitoredVehicleJourney']['VehicleLocation']['Latitude'])\n",
    "    dict['Stop'] = str(i['MonitoredVehicleJourney']['MonitoredCall']['StopPointName'])\n",
    "    dict['Status'] = str(i['MonitoredVehicleJourney']['MonitoredCall']['Extensions']['Distances']['PresentableDistance'])\n",
    "    datalist.append(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting a seed\n",
    "np.random.seed(10)\n",
    "\n",
    "#Making a random distribution\n",
    "Array = np.random.randn(2, 100)\n",
    "\n",
    "#Checking the shape of the array\n",
    "Array.shape\n",
    "\n",
    "#Making 100 random values, range 0 - 2000, setting them as integers\n",
    "(np.random.rand(100) * 2000).astype(int)\n",
    "\n",
    "#Sort data\n",
    "Data_Sort = np.sort(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sys Arg\n",
    "if len(sys.argv) != 3:\n",
    "    print(\"You did not enter the appropriate number of arguments. Please try again\")\n",
    "    sys.exit()\n",
    "\n",
    "Arg1 = python file\n",
    "Arg2 = sys.argv[1]\n",
    "Arg3 = sys.argv[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below is the code to create a random Gaussian distribution, with variables (mean, sd, sample size)\n",
    "Rand_100 = np.random.normal(10, 2, 1000)\n",
    "Rand_200 = np.random.normal(15, 3, 1000)\n",
    "\n",
    "#Plotting the histogram\n",
    "plt.hist(Rand_100, alpha=.5);\n",
    "plt.hist(Rand_200, alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Binomial distributions do not take the mean as an argument. You must figure out how to dictate the mean given the \n",
    "#arguments it wants (trials (n) and probabilities (p)). In this case, the mean = n*p. So if we want a distribution with \n",
    "#a mean of 5 with 1000 elements: (note: binomial yields only integers)\n",
    "\n",
    "Binom = np.random.binomial(10,.5,1000)\n",
    "\n",
    "#Normalized Binomial\n",
    "n = 10 \n",
    "p = 0.5\n",
    "binom = (np.random.binomial(n, p, 1000) - n*p) / np.sqrt(n*p*(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.random.poisson takes a single parameter, Lamda, which is the mean. It is bound to positive numbers. If the mean \n",
    "#is close to 0, the shape will be very asymmetrical... the further from 0 you get, the more symmetrical it will appear. \n",
    "#In a large number, the Poisson distribution begins to look like a Gaussian distribution\n",
    "\n",
    "Poi = np.random.poisson(1,1000)\n",
    "\n",
    "#Normalized Poisson\n",
    "lam = desired mean\n",
    "pois = (np.random.poisson(lam, 1000) - lam) * (1/np.sqrt(lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping for Increased Means & Plotting, from HW 5 assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "pois_KS_p_values = []\n",
    "pois_KS_s_values = []\n",
    "pois_AD_s_values = []\n",
    "pois_KL_s_values = []\n",
    "pois_CS_s_values = []\n",
    "pois_CS_p_values = []\n",
    "AD_threshold = 0.784\n",
    "\n",
    "for i,n in enumerate(mean_array):\n",
    "    lam = n \n",
    "    \n",
    "    #Create The Distributions\n",
    "    dist = (np.random.poisson(lam, 1000) - lam) * (1/np.sqrt(lam))\n",
    "    norm = np.random.randn(1000) \n",
    "    \n",
    "    #Determine the KS stats-values\n",
    "    KS_s_value = scipy.stats.kstest(dist,'norm')[0]\n",
    "    pois_KS_s_values.append(KS_s_value)\n",
    "    \n",
    "    #Determine the KS p-values\n",
    "    KS_p_value = scipy.stats.kstest(dist,'norm')[1]\n",
    "    pois_KS_p_values.append(KS_p_value)\n",
    "    \n",
    "    #Determine the AD stat-values\n",
    "    AD_s_value = scipy.stats.anderson(dist,'norm')[0]\n",
    "    pois_AD_s_values.append(AD_s_value)\n",
    "    \n",
    "    #Determine the KL statistic\n",
    "    pdfPois, poisBins, = np.histogram(dist, density=True)\n",
    "    poisBinCent = poisBins[:-1] + 0.5*(poisBins[1] - poisBins[0])\n",
    "    KL_s_value = scipy.stats.entropy(pdfPois, scipy.stats.norm.pdf(poisBinCent))\n",
    "    pois_KL_s_values.append(KL_s_value)  \n",
    "    \n",
    "    #Determine the Chi-Squ values\n",
    "    CS_s_value = scipy.stats.chisquare(pdfPois, scipy.stats.norm.pdf(poisBinCent))[0]\n",
    "    pois_CS_s_values.append(CS_s_value)\n",
    "    \n",
    "    #Determine the Chi-Squ p-values\n",
    "    CS_p_value = scipy.stats.chisquare(pdfPois, scipy.stats.norm.pdf(poisBinCent))[1]\n",
    "    pois_CS_p_values.append(CS_p_value)\n",
    "    \n",
    "    \n",
    "#Plotting\n",
    "\n",
    "#KS Test\n",
    "fig = pl.figure(figsize = (15,10))\n",
    "fig.add_subplot(221)\n",
    "pl.plot(mean_array, pois_KS_s_values, label='KS statistics')\n",
    "pl.plot(mean_array, pois_KS_p_values, label='KS p-value')\n",
    "pl.legend()\n",
    "\n",
    "#AD Test\n",
    "fig.add_subplot(222)\n",
    "pl.plot(mean_array, pois_AD_s_values,  label='AD statistics')\n",
    "pl.plot([mean_array[0], mean_array[-1]],[AD_threshold, AD_threshold], label=\"Threshold\")\n",
    "pl.ylim(0,10) #limit the y range or you cannot see the relevant part\n",
    "pl.legend()\n",
    "\n",
    "#KL Test\n",
    "fig.add_subplot(223)\n",
    "pl.plot(mean_array, pois_KL_s_values, label='K-L (entropy)')\n",
    "pl.legend()\n",
    "\n",
    "#Chi-Square Test\n",
    "fig.add_subplot(224)\n",
    "pl.plot(mean_array, pois_CS_s_values, label='Chi-Square statistic')\n",
    "pl.plot(mean_array, pois_CS_p_values, label='Chi-Square p-value')\n",
    "pl.ylim(0,1.1)\n",
    "pl.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "* A mathematical equation that describes data\n",
    "* All models are designed to be a simplification of the data, so it will only offer a partial understanding of the data\n",
    "* “Over-elaboration and over-parameterization is often the mark of mediocrity”\n",
    "\n",
    "\n",
    "## Model Diagnostics / Goodness of Fit\n",
    "\n",
    "### R-squared -- are my predictions close enough to the observations \n",
    "* Problematic for physical observations\n",
    "* Difference between point and model line, squared\n",
    "* Perfect model has r-squared at 1\n",
    "* WE SHOULD NEVER USE THIS, it only tells us how far we are from the line\n",
    "\n",
    "### Adjusted r-squared  -- r-squared, but accounts for number of degrees of freedom \n",
    "* Better to use than the r-squared\n",
    "* Not provided in statsmodels, because we use Chi-squared\n",
    "\n",
    "### Chi-Squared (chi2) -- are my model predictions close enough to the observations accounting for uncertainties in the data?\n",
    "* M- model prediction\n",
    "* X - observation\n",
    "* Sigma - uncertainty in the observation\n",
    "* Sum of ((m-x)^2 / sigma^2 ) for all observations * 1/DOF (degrees of freedom)\n",
    "* Ideal score is 1\n",
    "* The larger your X^2 value, the worse your model\n",
    "* If it goes below 1, you should be suspicious of overfitting or overestimating the uncertainties \n",
    "\n",
    "\n",
    "#### Is my model complete (are the residuals randomly distributed?)\n",
    "#### Is my model overfitting (chi2 - or better compare to a simpler model, LR ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLS\n",
    "\n",
    "y = df['Number of mass shootings']\n",
    "x = df['Average Total Civilian Firearms']\n",
    "\n",
    "rmO = sm.OLS(endog = y, exog = x).fit()\n",
    "yerr=np.abs(y)**0.5\n",
    "\n",
    "pl.plot(x,y,'.')\n",
    "pl.errorbar(x,y,yerr=yerr, fmt='.')\n",
    "pl.plot(x,rmO.fittedvalues,'-')\n",
    "pl.xlabel(\"Number of civilian firearms (normalized)\")\n",
    "pl.ylabel(\"Number of mass shootings\")\n",
    "pl.title(\"Mass shootings per civilian firearms\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WLS\n",
    "rmW = sm.WLS(endog = y, exog = x, weights = 1/(yerr+1) ).fit()\n",
    "yerr=np.abs(y)**0.5\n",
    "\n",
    "pl.plot(x,y,'.')\n",
    "pl.errorbar(x,y,yerr=np.abs(y)**0.5, fmt='.')\n",
    "pl.plot(x,rmW.fittedvalues,'-')\n",
    "pl.xlabel(\"Number of civilian firearms (normalized)\")\n",
    "pl.ylabel(\"Number of mass shootings\")\n",
    "pl.title(\"Mass shootings per civilian firearms\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seaborn\n",
    "sns.regplot(x, y, data=df)\n",
    "pl.title(\"Mass shootings per civilian firearms\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Polyfit\n",
    "\n",
    "fit = np.polyfit(x,y,1)\n",
    "yfit = (fit[0] * x) + fit[1]\n",
    "\n",
    "pl.plot(x,y,'.')\n",
    "yerr=np.abs(y)**0.5\n",
    "pl.errorbar(x,y,yerr=np.abs(y)**0.5, fmt='.')\n",
    "pl.plot(x,rmO.fittedvalues,'-')\n",
    "pl.plot(x,yfit,'-');\n",
    "pl.xlabel(\"Number of civilian firearms (normalized)\")\n",
    "pl.ylabel(\"Number of mass shootings\")\n",
    "pl.title(\"Mass shootings per civilian firearms\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Influence Plot\n",
    "\n",
    "sm.graphics.influence_plot(rmO, alpha  = 0.05, criterion=\"cooks\")\n",
    "pl.title(\"Mass shootings per civilian firearms\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linmodel.summary()\n",
    "#gives you a breakdown of the model\n",
    "\n",
    "linmodel.fittedvalues\n",
    "#necessary for plotting\n",
    "\n",
    "linmodel.params\n",
    "\n",
    "#weighted model\n",
    "wmodel = sm.WLS(endog=y,exog=sm.add_constant(x),weights=1.0/yerr).fit()\n",
    "\n",
    "#Create a line model with noise\n",
    "np.random.seed(100)\n",
    "def linefunc(x, m=1, b=0):\n",
    "    \"\"\"Defines a line based on parameters\"\"\"\n",
    "    y = m * x + b\n",
    "    \n",
    "    #adding noise\n",
    "    yerr =  np.random.randn(len(x)) * sqrt(y.mean()) / 3\n",
    "    \n",
    "    return y + yerr\n",
    "\n",
    "\n",
    "#Using ~\n",
    "lm = sm.ols(\"y ~ x\", data = df_).fit()\n",
    "#This sets y proportional to x, whatever that means\n",
    "\n",
    "https://github.com/fedhere/PUI2017_fb55/blob/master/Lab5_fb55/line_fit_and_residuals.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 - create dictionary to make a new variable, x2\n",
    "df_ = pd.DataFrame({'y': ln, 'x': x, 'x2': x**2})\n",
    "\n",
    "#2 - do this:\n",
    "qm2 = sm.ols(\"y ~ x2\", data = df_).fit()\n",
    "\n",
    "#OR SIMPLY:\n",
    "qm = sm.ols(\"y ~ x + I(x**2)\", data = df_).fit()\n",
    "#This tells you to multiply x by the identity matrix. This is the method to get a quadratic regression\n",
    "\n",
    "\n",
    "#Because the quadratic fit gave us a worse adj. R fit (when looking at qm.summary()), we can assert that the \n",
    "#line fit was better. Or the opposite, if adj.Rfit is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "* The probability of the model, given your data\n",
    "* We don’t know the mean and standard deviation, but we know the x values\n",
    "* We can figure them out by maximizing the likelihood\n",
    "* Goal is to find the mew and sigma that maximizes the Likelihood \n",
    "* However, because it is mathematically convenient, we actually try to minimize the Likelihood \n",
    "\n",
    "#### Logarithm : wherever x grows, the log grows. Wherever x shrinks, the log shrinks\n",
    "* Thus the maximum of x is the maximum of the log\n",
    "* We can take the log of the likelihood in order to:\n",
    "* shrink the values (likelihoods tend to be very large and very spread out)\n",
    "* Allow us to take the derivative to identify the minimum the log likelihood\n",
    "* Stats models doesn’t even bother to give you the likelihood, it just gives you the log likelihood\n",
    "\n",
    "### Likelihood Ratio Tests\n",
    "* LR = -2log e L(model1 ) / L (model 2)\n",
    "* Model 1 is the simpler model\n",
    "* If the models are nested \n",
    "* m1 = bx + c, m2 = ax^2 +bx +c\n",
    "* M1 is dested in M2 because it exists inside of it\n",
    "* Difference in number of DOF is the difference between the number of paramaters\n",
    "* Null hypothesis: The simple model is better than the more complex model\n",
    "* I.e. the null states that you are over fitting\n",
    "* If the LR is above the critical value, you can reject the Null\n",
    "\n",
    "#### AIC and BIC\n",
    "* Parameters will decrease the better your model is fit\n",
    "* Eventually will flatten out, like an asymptote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.llf\n",
    "\n",
    "-2 * (model1.llf - model2.llf)\n",
    "\n",
    "#Built In Likelihood Ratio test\n",
    "qm.compare_lr_test(lm)\n",
    "#where qm is the more complex model and lm is the simpler, NESTED model\n",
    "#MUST BE NESTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Models & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Including loglog=True to plot the log values\n",
    "ax = dfCut.plot(kind='scatter', y='nrgTotal', x='UnitsTotal', marker='.',  figsize=(16, 14), loglog=True)\n",
    "\n",
    "#Creating a model of the log of data\n",
    "X1 = sm.add_constant(np.log10(dfCut['X VALUE']))\n",
    "linmodelLog = sm.OLS(np.log10(dfCut['Y VALUE']), X1, missing='drop').fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Statistics\n",
    "\n",
    "#### The Principle of Falsifiability\n",
    "* Instead of directly testing the hypothesis, we test the opposite (compliment) of it -- the null hypothesis ($H_0$) (pronounced ‘H-not’)\n",
    "* We call the hypothesis originating from our original idea the Alternative Hypothesis ($H_\\alpha$)\n",
    "* If we can reject the Null Hypothesis, the Alt. Hypothesis holds (for now)\n",
    "\n",
    "#### Central Limit Theorem \n",
    "* If I have N elements drawn from some population with a parent distribution:\n",
    "* The mean and SD of the sample, in the limit of N → infinity, the distribution of the sample’s mean\n",
    "* If we measure something with an oddly shaped distribution, and we want to measure the average statistics\n",
    "* If the sample is large enough, and as the samples grown, the means of the samples will approach a Gaussian distribution\n",
    "* The larger the sample, the more likely it is that you are calculating the mean of the parent distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the mean\n",
    "mean(Binom)\n",
    "#or\n",
    "Binom.mean\n",
    "\n",
    "#Finding the SD\n",
    "Binom.std\n",
    "std(Binom)\n",
    "\n",
    "#discrete data arise fom a counting process, while continuous data arise from a measuring process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scipy Statistical Tests\n",
    "\n",
    "#### 2-tailed test\n",
    "* Null Hypothesis states that there is no relationship, no difference, nada\n",
    "* You can falsify it if you have, in fact, seen a difference\n",
    "\n",
    "#### 1-tailed test\n",
    "* Null hypothesis states there is a directional relationship, i.e. one variable will be larger or smaller\n",
    "\n",
    "#### P-value\n",
    "* Measure of the probability that the result you observed could have been observed by chance under the Null Hypothesis\n",
    "\n",
    "#### Errors\n",
    "\n",
    "##### Type I\n",
    "* False positive\n",
    "* We reject the null incorrectly\n",
    "* Important message is marked as spam\n",
    "\n",
    "##### Type II\n",
    "* False negative\n",
    "* We accept the null incorrectly\n",
    "* Spam makes its way into our inbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z Test\n",
    "\n",
    "* Useful if we know the parameters of the population\n",
    "* If data follows symmetric Gaussian distribution\n",
    "* Z = ($\\mu$ - m) / ($\\sigma$/ sqrt(N)) = sqrt(sample size) * ($\\mu$ - m) / $\\sigma$\n",
    "* If there is no difference, the Z score will be 0\n",
    "* More likely to get this by increasing the size of the sample\n",
    "* The Z test will tell you the probability for rejecting the null in your hypothesis\n",
    "* Z-test for the mean, and z-test for the proportion\n",
    "* Both numbers follow a Gaussian distribution\n",
    "* Mean = 0, SD = 1\n",
    "* Because it’s Gaussian distributed, we know what the likelihood of getting a specific number is under the null 0.05 is equal to a z value of 1.96… this is the number we want to be higher than\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ztest(m):\n",
    "    \"\"\"Runs the z test\"\"\"\n",
    "    z = (mean - m) / sigma * sqrt(N)\n",
    "    return z\n",
    "\n",
    "#Making an array of all the z-scores\n",
    "zscores = []\n",
    "for m in mymeans:\n",
    "    zscore = ztest(m)\n",
    "    zscores.append(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z test compares the standard deviation of the expected distribution and the observed result. It tells you \n",
    "literally how many standard deviations from the tail an observation is, under the assumption of normality\n",
    "\n",
    "** z score: how many standard deviation away from the population parameter is my statistic? **\n",
    "\n",
    "** $H_0$ = The sample comes from the population distribution and is not significantly different**\n",
    "\n",
    "\n",
    "$z=\\frac{P_1-P_0}{\\sigma}$\n",
    " \n",
    "if $p<\\alpha$ : reject $H_0$\n",
    "\n",
    "IMPORTANT!! note that this P in the bottom line of the table is not the p-value, but rather: \n",
    "\n",
    "p-value = 1-P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student’s t test\n",
    "* When we do not have the population parameters\n",
    "* Symmetric, but not gaussian \n",
    "* Compare two samples to each other\n",
    "* Parametric distribution, based on n\n",
    "* n = number of observations\n",
    "* q = 1- alpha\n",
    "* Not shaped like a gaussian distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's Chi Square\n",
    "\n",
    "* Is there a difference between means or population and sample?\n",
    "* Not symmetric\n",
    "* Changes shape dramatically based on its one parameter:\n",
    "* Degrees of freedom = number of observations - independent variables - 1\n",
    "* P0 = control percent\n",
    "* P1 = test percent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are basically two types of random variables and they yield two types of data: numerical and categorical. A chi square (X2) statistic is used to investigate whether distributions of categorical variables differ from one another. Basically categorical variable yield data in the categories and numerical variables yield data in numerical form. Responses to such questions as \"What is your major?\" or Do you own a car?\" are categorical because they yield data such as \"biology\" or \"no.\" In contrast, responses to such questions as \"How tall are you?\" or \"What is your G.P.A.?\" are numerical. Numerical data can be either discrete or continuous. The table below may help you see the differences between these two variables. http://math.hws.edu/javamath/ryan/ChiSquare.html\n",
    "\n",
    "** The chisq statistics tests the statistics calculated as : **\n",
    "\n",
    "$\\chi^2 = \\sum_{i} \\frac{(observation_i - expectation_i)^2}{expectation_i}$\n",
    "\n",
    "against a chi sq distribution.\n",
    "If we talk about sample fractions  that is \n",
    "\n",
    "$\\chi^2 = \\sum_i \\frac{(f_{i,observed} - f_{i,expectated})^2}{f_{i,expected}}$\n",
    "\n",
    "Where _i_ indicates the sum over _each cell_.\n",
    "turns out this quantity is distributed according to a chi square distribution, so if i get the $\\chi^2$ statistics i can compare it to the full chisq distribution and see how far in the tail it is\n",
    "\n",
    "The trickiest part (but not that tricky) is to figure out how to construct the table of values. please see Statistics In a Nutshell Chapter 4, for our data for example: Thisis called a CONTINGENCY TABLE\n",
    " \n",
    "|                 |     success         | failure|    |               \n",
    "|-----------------|:-------------------:|:-------------------:|---------------------------|\n",
    "| test sample     | number of successes in test    | number of failures in test    | number members of test sample |\n",
    "| control sample  | number of successes in control | number of failures in control | number members of control sample| \n",
    "|                 | total successes                |  total failures               | number of all members         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EVALUATING THE CHI-SQUARE\n",
    "def evalChisq(values):\n",
    "    '''Evaluates the chi sq from a contingency value\n",
    "    Arguments:\n",
    "    values: 2x2 array or list, the contingengy table\n",
    "    '''\n",
    "    if not (len(values.shape) == 2 and values.shape == (2,2)):\n",
    "        print (\"must pass a 2D array\")\n",
    "        return -1\n",
    "    values = np.array(values)\n",
    "    E = np.empty_like(values)\n",
    "    for j in range(len(values[0])):\n",
    "        for i in range(2):\n",
    "            \n",
    "            E[i][j] = ((values[i,:].sum() * values[:,j].sum()) / \n",
    "                        (values).sum())\n",
    "    return ((values - E)**2 / E).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number must be compared to the chi sq distribution. You must calculate the number of degrees of freedom for this experiment. Generally: \n",
    "\n",
    "** DOF = Number of observations - number of Independent Variables **\n",
    "\n",
    "Critical value at p=0.05 is 3.84, so you reject the null if your evalChisq output > 3.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Similarity\n",
    "\n",
    "* Determining whether they come from the same parent distribution\n",
    "* We look at the Cumulative Distribution Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KS Test\n",
    "\n",
    "* Depends on shape of histogram, along with bin size. Bin size is important -- the number you get will be different depending on the bin width. Binning helps suppress some of the noise, so even if the number isn’t perfect, it is useful\n",
    "* Does not answer whether the samples are similar. Answers how likely it is that the samples were extracted from the same parent distribution.\n",
    "* Typically work on pairs of observations\n",
    "* Can’t do it on two samples of different sizes\n",
    "* Have to pair the samples up\n",
    "* KS works best in assessing central tendencies, i.e. close to the mean\n",
    "* Not good for differences at the tails of the distribution\n",
    "* Non parametric -- works with whatever the true shape of the distribution is, whether you can describe it with an equation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $H_0$: The two samples come from a common distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.ks_2samp(day_val, night_val) #both imputs are arrays, created by the .values function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output - Ks_2sampResult(statistic=0.022754872040973356, pvalue=1.8980926024220832e-135)\n",
    "\n",
    "#### Reject the null, indicating that the two samples are not pulled from the same parent distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anderson Darling\n",
    "\n",
    "* Has more power at the tails\n",
    "* Can extend to more powers of dimensions, KS cannot\n",
    "* Assumes the original distribution is Gaussian\n",
    "* If they come from a binomial, it doesn’t work\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $H_0$: The sample comes from a normal distribution \n",
    "\n",
    "Critical value at 0.05 = somewhere around 0.736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.anderson(zscores, 'norm')) #takes one array, matches against a normal distriution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AndersonResult(statistic=0.48620799130414127, critical_values=array([ 0.555,  0.632,  0.759,  0.885,  1.053]), significance_level=array([ 15. ,  10. ,   5. ,   2.5,   1. ])))\n",
    "\n",
    "#### Based on the AD test, we are unable to reject $H_0$ because the critical value (0.486) is not higher than the necessary critical value for our selected p-value (0.736). This implies that the z-score distribution comes from a Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepping and running the KL test for a Distribution\n",
    "pdfPois, poisBins, = np.histogram(pois, density=True)\n",
    "poisBinCent = poisBins[:-1] + 0.5*(poisBins[1] - poisBins[0])\n",
    "\n",
    "scipy.stats.entropy(pdfPois, scipy.stats.norm.pdf(poisBinCent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Test - Because out KL statistic (0.0049) is  small we  can assert that the difference in entropy between our sample distribution and our Gaussian parameter is small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing For Correlation\n",
    "\n",
    "* If you measure enough things, you’ll find 2 things that correlate \n",
    "* Correlation does not mean causation\n",
    "* Related, correlated, influenced by each other, or influences by some other factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson's\n",
    "\n",
    "* r = is bound by 1 and -1\n",
    "* 1 means maximum correlation\n",
    "* -1 means inverse correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(70)\n",
    "Cor_day = np.random.choice(dfDur_day['tripduration'], size=20000)\n",
    "Cor_night = np.random.choice(dfDur_night['tripduration'], size=20000)\n",
    "Day_sort = np.sort(Cor_day)\n",
    "Night_sort = np.sort(Cor_night)\n",
    "\n",
    "scipy.stats.pearsonr(Day_sort , Night_sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data must be sorted, and must be of equal sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample rejection - \"Based on the Pearson correlation coefficient (0.70), there is a strong correlation between trip length for trips beginning during the day and trips beginning during the night. Because our p-value is very small, we can assume that the correlation coefficient is significant. Thus, we can reject $H_0$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's\n",
    "\n",
    "* Pearson’s, but for ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar to Pearson's, data must be sorted\n",
    "\n",
    "scipy.stats.spearmanr(Sort_M, Sort_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUI2016_Python2",
   "language": "python",
   "name": "pui2016_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
